{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_experimental.tools import PythonREPLTool\n",
    "from Libs.libs import llm\n",
    "from langchain_core.tools import tool\n",
    "from Libs.libs import *\n",
    "# from Tools.Schema import DateModel\n",
    "from typing import Literal\n",
    "import pandas as pd\n",
    "\n",
    "from Libs.libs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "collection_name: str = os.getenv(\"RAG_GYM\")\n",
    "\n",
    "vector_store=Milvus(embedding_function=embeddings,connection_args={ \"host\":host, \"port\":port},collection_name=collection_name)\n",
    "@tool\n",
    "def rag_gym(query: str):\n",
    "    \"\"\"Used to answer the queries related to TATA Punch EV and its variants: Medium Range, Long Range.\"\"\"\n",
    "    context = vector_store.similarity_search(query, k=4)\n",
    "\n",
    "    pprint.pprint(context)\n",
    "    template = \"\"\"You are given a context and a human query.\n",
    "    Do not ever answer using your own knowledge base. \n",
    "\n",
    "    Human query is: {query}\n",
    "    Context is :{context}\n",
    "    Analyze the human query and context and answer the question related \n",
    "    Always ignore the place name mentioned in the query for each and every qeury as the place name does not matter .\n",
    "    Your response should be precise, concise and not contain any irrelevant information.\n",
    "\n",
    "    \n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template=template)\n",
    "\n",
    "    chain = RunnableMap({\n",
    "        'query': lambda x: x['query'],\n",
    "        \"context\": lambda x: vector_store.similarity_search(x['query'], k=6),\n",
    "    }) | prompt | llm | string_parser\n",
    "\n",
    "    res = chain.invoke({\n",
    "        \"query\": query,\n",
    "        })\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def availabilityy(desired_date: str, doctor_name: str):\n",
    "    desired_date_split = desired_date.split(\"T\") or desired_date\n",
    "    print(\"desired date is:\", desired_date)\n",
    "    df = pd.read_csv(f\"./data/syntetic_data/availability.csv\")\n",
    "    date = desired_date_split[0]\n",
    "    time = desired_date_split[1] if len(desired_date_split) > 1 else \"\"\n",
    "    print(time)\n",
    "    # query = doctor_name+ \" for \" + date + \" at time: \" +time\n",
    "    query = doctor_name + \" for \" + date + \",\" + time\n",
    "    # print(f\"{df['date_slot'].apply(lambda x: x.split(' '))}\", query)\n",
    "    availability = pd.DataFrame()\n",
    "    if time != \"\":\n",
    "        availability = df[\n",
    "            (df[\"date_slot\"].apply(lambda x: x.split(\" \")[0]) == date)\n",
    "            & (df[\"date_slot\"].apply(lambda x: x.split(\" \")[1]) == time)\n",
    "            & (doctor_name == df[\"doctor_name\"])\n",
    "            & (df[\"is_available\"] == True)\n",
    "        ]\n",
    "        if availability.empty:\n",
    "            availability = df[\n",
    "                (df[\"date_slot\"].apply(lambda x: x.split(\" \")[0]) == date)\n",
    "                & (doctor_name == df[\"doctor_name\"])\n",
    "                & (df[\"is_available\"] == True)\n",
    "            ]\n",
    "        return query, availability\n",
    "    availability = df[\n",
    "        (df[\"date_slot\"].apply(lambda x: x.split(\" \")[0]) == date)\n",
    "        & (doctor_name == df[\"doctor_name\"])\n",
    "        & (df[\"is_available\"] == True)\n",
    "    ]\n",
    "    print(\"---->>>>>>>\", availability, \">>>-----\", query)\n",
    "    return query, availability\n",
    "\n",
    "\n",
    "# def check_availability_by_doctor(self):\n",
    "@tool\n",
    "def check_availability_by_doctor(\n",
    "    desired_date: str,\n",
    "    doctor_name: Literal[\n",
    "        \"kevin anderson\",\n",
    "        \"robert martinez\",\n",
    "        \"susan davis\",\n",
    "        \"daniel miller\",\n",
    "        \"sarah wilson\",\n",
    "        \"michael green\",\n",
    "        \"lisa brown\",\n",
    "        \"jane smith\",\n",
    "        \"emily johnson\",\n",
    "        \"john doe\",\n",
    "    ],\n",
    "):\n",
    "    # desired_date, doctor_name:Literal['kevin anderson','Ajeet','Rajesh','Shiva','Ganesh',]):\n",
    "    \"\"\"\n",
    "    Check the availability of the doctor with the name of the doctor and date of availability provided\n",
    "    \"\"\"\n",
    "    print()\n",
    "    # print(desired_date)\n",
    "    print(\"from check availability by doctor tool\")\n",
    "    query, availability = availabilityy(\n",
    "        desired_date=desired_date, doctor_name=doctor_name\n",
    "    )\n",
    "    print(\"Tool entered with the query\", query)\n",
    "    # print(availability)\n",
    "    template = \"\"\"\n",
    "    You have to answer the user query which is about the availability of a doctor inquired by patient. You are also given a context that contains the doctors available in that day and a time slot. Analyze and return the doctor name, date and available time slot. \n",
    "        Context inside double backticks:`{context}`\n",
    "        Question inside triple backticks:{question}\n",
    "        Response in markdown format without any backticks and in the context phrase just answer what is asked and if the doctor is not available for a particular time slot, recomend the other time slots.\n",
    "\n",
    "        Your response should be very much conversational in such a way that you are a receptionist talking to a patient who is here to schedule an appointment.\n",
    "        \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    # print(\"context>>>>>>>>>>>>>>>>\",database.similarity_search(query,k=5))\n",
    "    chain = (\n",
    "        RunnableMap(\n",
    "            {\"context\": lambda x: availability, \"question\": lambda x: x[\"question\"]}\n",
    "        )\n",
    "        | prompt\n",
    "        | llm\n",
    "        | string_parser\n",
    "    )\n",
    "    result = chain.invoke({\"question\": query})\n",
    "    print(\"------------\")\n",
    "    mdprint(result)\n",
    "    print(\"------------\")\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def availability_by_specialization(desired_date:str, specialization:str):\n",
    "    desired_date_split = desired_date.split(\"T\") or desired_date\n",
    "\n",
    "    date =  desired_date_split[0] \n",
    "    time = desired_date_split[1] if len(desired_date_split) > 1 else \"\" \n",
    "    df = pd.read_csv(f\"./data/syntetic_data/availability.csv\")\n",
    "    rows = df[(df['date_slot'].apply(lambda input: input.split(' ')[0]) == date) & (df['specialization'] == specialization) & (df['is_available'] == True)].reset_index()\n",
    "    available_specialization = rows[['date_slot', 'specialization', 'doctor_name']] \n",
    "      \n",
    "    query = specialization+ \" for \" + date + \",\" +time\n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "    return query, available_specialization\n",
    "    \n",
    "\n",
    "# def check_availability_by_specialization(self):\n",
    "\n",
    "@tool\n",
    "def check_availability_by_specialization(desired_date:str, specialization:Literal[\"general_dentist\", \"cosmetic_dentist\", \"prosthodontist\", \"pediatric_dentist\",\"emergency_dentist\",\"oral_surgeon\",\"orthodontist\"]):\n",
    "    \"\"\"\n",
    "    Check the availability of the doctor with specialization and date provided.\n",
    "    \"\"\"\n",
    "    # print(self.senderId)\n",
    "    print(\"from check availability by doctor tool\")\n",
    "    # specialization = \"orthodontist\"\n",
    "    # desired_date = \"2024-09-03\"\n",
    "    \n",
    "    query, available_specialization = availability_by_specialization(desired_date=desired_date, specialization=specialization)\n",
    "    print(\"Tool entered with the query\", query)\n",
    "\n",
    "    template = \"\"\"\n",
    "    You have to answer the user query in which patient is searching for a doctor of a particular specialization which is given in the query. You are also given a context that contains the doctors available in that day and a time slot. Analyze and return the doctor name, date and available time slot along with the specialization. \n",
    "        Context inside double backticks:`{context}`\n",
    "        Question inside triple backticks:{question}\n",
    "    Response in markdown format without any backticks and in the context phrase just answer what is asked and if the doctor is not available for a particular time slot, recomend the other time slots.\n",
    "    Your response should be very much conversational in such a way that you are a receptionist talking to a patient who is here to schedule an appointment.\n",
    "\n",
    "        \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    # print(\"context>>>>>>>>>>>>>>>>\",database.similarity_search(query,k=5))\n",
    "    chain = RunnableMap({\n",
    "        \"context\":lambda x: available_specialization,\n",
    "        \"question\": lambda x: x['question']\n",
    "    }) | prompt | llm | string_parser\n",
    "    result = chain.invoke({'question':query})\n",
    "    mdprint(result)\n",
    "    return result\n",
    "    # return check_specialization\n",
    "    \n",
    "        \n",
    "\n",
    "    # @tool\n",
    "    # def book_appointment( doctor_name:str, desired_time:str):\n",
    "    #     \"\"\"\n",
    "    #     Schedule or Book an appointment for a doctor\n",
    "    #     \"\"\"\n",
    "    #     print(\"Booking appointment tool\")\n",
    "    #     # app = patient_name + \" \" + desired_date + \" \"+ doctor_name + \" \" + specialization + \" \" + desired_time\n",
    "    #     print(\"app\")\n",
    "        \n",
    "    #     return \"Booked \"\n",
    "\n",
    "\n",
    "    # @tool\n",
    "    # def reschedule_appointment(patient_name:str,patient_id:str,desired_date:str, doctor_name:str, specialization:str, desired_time:str):\n",
    "    #     \"\"\"\n",
    "    #     To reschedule an appointment which has already been scheduled to a new date\n",
    "    #     \"\"\"\n",
    "    #     pass\n",
    "\n",
    "@tool\n",
    "def book_appointment(desired_date:str, desired_time:str,  doctor_name:str):\n",
    "    \"\"\"\n",
    "    When the user wants to book an appointment for a doctor at a specified time.\n",
    "\n",
    "    \"\"\"\n",
    "    # print(self.senderId)\n",
    "    patient_id = \"Check123Check\"\n",
    "    patient_name = \"Ajeet Acharya\"\n",
    "    print(\"book tool activated\", desired_date, \" \", desired_time)\n",
    "    availability = pd.DataFrame()\n",
    "    specializations = pd.DataFrame()\n",
    "    # if specialization ==\"\":\n",
    "    desired_date_time = f\"{desired_date}T{desired_time}\"\n",
    "    _, availability = availabilityy(desired_date=desired_date_time, doctor_name=doctor_name)\n",
    "    availability_new = availability[[\"date_slot\",\"specialization\",\"doctor_name\"]]\n",
    "    if len(availability) ==1:\n",
    "        date_id_booking = availability.index[0]\n",
    "        date_slot_booking = availability['date_slot'].iloc[0]\n",
    "        time, date = date_slot_booking.split(\" \")[0],  date_slot_booking.split(\" \")[1]\n",
    "        time = desired_time.split(\":\")\n",
    "        final_time = f\"{time[0]}:{time[1]}\"\n",
    "        if final_time == desired_time:\n",
    "            doctor_name_booking = availability['doctor_name'].iloc[0]\n",
    "            specialization_booking = availability['specialization'].iloc[0]\n",
    "            data = pd.DataFrame({\n",
    "                \"patient_id\": [patient_id],\n",
    "                \"doctor_time_id\": [date_id_booking],\n",
    "                \"patient_name\": [patient_name],\n",
    "                \"doctor_name\": [doctor_name_booking],\n",
    "                \"appointment_date\": [desired_date],\n",
    "                \"appointment_time\": [final_time],\n",
    "                \"specialization\": [specialization_booking],\n",
    "            }\n",
    "        )\n",
    "            \n",
    "            message = write_to_excel(data=data, date_id_booking=date_id_booking, patient_id=patient_id)\n",
    "            print(\"Data written to Excel successfully!\")\n",
    "            return message\n",
    "    else: \n",
    "        return f\"The time slot {desired_time} is not available for {doctor_name}. Please select one of the.\\n {availability_new}\"\n",
    "    # return book_appoint\n",
    "\n",
    "def write_to_excel(data, date_id_booking, patient_id):\n",
    "    data.to_excel(\"./Appointment/doctor_appointments.xlsx\")  # 'index=False' prevents writing row numbers\n",
    "    book_appointment_path = r\"data/syntetic_data/availability.csv\"\n",
    "    df = pd.read_csv(book_appointment_path)\n",
    "    column_no = date_id_booking-2\n",
    "    df.loc[column_no, 'is_available'] = False \n",
    "    df.loc[column_no,'patient_to_attend'] = \"Ajeet123\"\n",
    "    df.to_csv(book_appointment_path, index=False)\n",
    "    return \"Booking has been made sucessfully\"\n",
    "\n",
    "\n",
    "\n",
    "# def reschedule(self):\n",
    "@tool\n",
    "def reschedule(\n",
    "        desired_date: str,\n",
    "        doctor_name: str,\n",
    "        desired_time: str,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        To reschedule an appointment which has already been scheduled to a new date\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        # pass\n",
    "        patient_id = \"Check123Check\"\n",
    "        print(\"reschedule tool activated\", desired_date, \" \", desired_time)\n",
    "        print(\"patient_id\", patient_id)\n",
    "        availability = pd.DataFrame()\n",
    "        appointments_df = pd.read_excel(\"./Appointment/doctor_appointments.xlsx\")\n",
    "        appointment_to_reschedule = appointments_df[\n",
    "            (appointments_df[\"patient_id\"] == patient_id)\n",
    "        ]\n",
    "        if appointment_to_reschedule.empty:\n",
    "            return f\"Error: No appointment found for patient {patient_id}.\"\n",
    "        original_date = f\"{appointment_to_reschedule['appointment_date'].iloc[0]} {appointment_to_reschedule['appointment_time'].iloc[0]}\"\n",
    "        print(\"original date\", original_date)\n",
    "        # doctor_name = appointment_to_reschedule[\"doctor_name\"].iloc[0]\n",
    "        # if specialization ==\"\":\n",
    "        desired_date_time = f\"{desired_date}T{desired_time}\"\n",
    "        _, availability = availabilityy(\n",
    "            desired_date=desired_date_time, doctor_name=doctor_name\n",
    "        )\n",
    "        availability_new = availability[[\"date_slot\", \"specialization\", \"doctor_name\"]]\n",
    "        print(\"availability\", availability)\n",
    "        print(\"availability_new--->\", availability_new)\n",
    "        if not availability.empty:\n",
    "            appointments_df.loc[\n",
    "                appointments_df[\"patient_id\"] == patient_id,\n",
    "                [\"appointment_time\", \"appointment_date\"],\n",
    "            ] = [desired_time, desired_date]\n",
    "\n",
    "            # Write the updated appointment back to Excel\n",
    "            appointments_df.to_excel(\"./Appointment/doctor_appointments.xlsx\", index=False)\n",
    "\n",
    "            message = update_availability_csv(\n",
    "                original_date=original_date,\n",
    "                desired_date=desired_date,\n",
    "                desired_time=desired_time,\n",
    "                patient_id=patient_id,\n",
    "            )\n",
    "\n",
    "            return f\"Appointment rescheduled successfully for {desired_date} at {desired_time}.\\n{message}\"\n",
    "        else:\n",
    "            return f\"Error: The time slot {desired_time} is not available for {doctor_name}. Please choose another slot.\"\n",
    "    # return reschedule_appointment\n",
    "\n",
    "def update_availability_csv(\n",
    "    original_date, desired_date, desired_time,patient_id\n",
    "):\n",
    "\n",
    "    availability_csv_path = r\"data/syntetic_data/availability.csv\"\n",
    "    df = pd.read_csv(availability_csv_path)\n",
    "    desired_date_time = f\"{desired_date} {desired_time}\"\n",
    "\n",
    "    df.loc[ df[\"date_slot\"].str.contains(original_date),\n",
    "        [\"is_available\", \"patient_to_attend\"]\n",
    "    ] = [True, \"\"]\n",
    "\n",
    "    # Update the new time slot to be unavailable (False) and assign the patient to attend\n",
    "    df.loc[df[\"date_slot\"].str.contains(desired_date_time),\n",
    "        [\"is_available\", \"patient_to_attend\"],\n",
    "    ] = [False, patient_id]\n",
    "\n",
    "    # Write the updated DataFrame back to the CSV file\n",
    "    df.to_csv(availability_csv_path, index=False)\n",
    "\n",
    "    return \"Doctor's availability updated successfully.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools_available = [check_availability_by_specialization, check_availability_by_doctor]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "def create_agent(llm, tools, system_message: str):\n",
    "    \"\"\"Create an agent.\"\"\"\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\n",
    "                \"system\",\n",
    "                \"You are a helpful AI assistant, collaborating with other assistants.\"\n",
    "                \" Use the provided tools to progress towards answering the question.\"\n",
    "                \" If you are unable to fully answer, that's OK, another assistant with different tools \"\n",
    "                \" will help where you left off. Execute what you can to make progress.\"\n",
    "                \" If you or any of the other assistants have the final answer or deliverable,\"\n",
    "                \" prefix your response with FINAL ANSWER so the team knows to stop.\"\n",
    "                \" You have access to the following tools: {tool_names}.\\n{system_message}\",\n",
    "            ),\n",
    "            MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        ]\n",
    "    )\n",
    "    prompt = prompt.partial(system_message=system_message)\n",
    "    prompt = prompt.partial(tool_names=\", \".join([tool.name for tool in tools]))\n",
    "    return prompt | llm.bind_tools(tools)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def agent_node(state, agent, name):\n",
    "    result = agent.invoke(state)\n",
    "    return {\n",
    "        \"messages\": [HumanMessage(content=result[\"messages\"][-1].content, name=name)]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervisor agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel\n",
    "from typing import Literal\n",
    "\n",
    "members = [\"Appointments\", \"Rag\"]\n",
    "system_prompt = (\n",
    "    \"You are a supervisor tasked with managing a conversation between the\"\n",
    "    \" following workers:  {members}. Given the following user request,\"\n",
    "    \" respond with the worker to act next. Analyze the user query well and then choose members. You do not need to go to all the members for a query When finished,\"\n",
    "    \" respond with FINISH. Only invoke one worker at once.\"\n",
    ")\n",
    "# Our team supervisor is an LLM node. It just picks the next agent to process\n",
    "# and decides when the work is completed\n",
    "options = [\"FINISH\"] + members\n",
    "\n",
    "\n",
    "class routeResponse(BaseModel):\n",
    "    next: Literal[\"Appointments\", \"Rag\", \"FINISH\"]\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Given the conversation above, who should act next?\"\n",
    "            \" Or should we FINISH? Select one of: {options}\",\n",
    "        ),\n",
    "    ]\n",
    ").partial(options=str(options), members=\", \".join(members))\n",
    "\n",
    "\n",
    "# llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "\n",
    "def supervisor_agent(state):\n",
    "    supervisor_chain = prompt | llm.with_structured_output(routeResponse)\n",
    "    return supervisor_chain.invoke(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Tools.tools_init_ import GetCustomTools\n",
    "\n",
    "toolsInstance = GetCustomTools()\n",
    "tools_available = toolsInstance.get_tools()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x159ca41d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import functools\n",
    "import operator\n",
    "from typing import Sequence\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "\n",
    "# The agent state is the input to each node in the graph\n",
    "\n",
    "\n",
    "\n",
    "Appointments = create_react_agent(llm, tools=[check_availability_by_doctor, check_availability_by_specialization,book_appointment, reschedule ], )\n",
    "                                #   inter=)\n",
    "# Appointments = create_agent(llm=llm,tools=[check_availability_by_doctor, check_availability_by_specialization,book_appointment, reschedule ],     system_message=\"You are an agent who is responsible for checking the availability of doctor based on doctor's name or specialization , booking appointments or rescheduling appointments of patients.\",\n",
    "# Rag = create_agent(llm=llm,tools=[rag_gym],system_message=\"You are an agent who is responsible for answering questions related to TATA Punch Ev.\",)\n",
    "Rag = create_react_agent(llm, tools=[rag_gym])\n",
    "Appointments_node = functools.partial(agent_node, agent=Appointments, name=\"Appointments\")\n",
    "Rag_node = functools.partial(agent_node, agent=Rag, name=\"Rag\")\n",
    "\n",
    "# # NOTE: THIS PERFORMS ARBITRARY CODE EXECUTION. PROCEED WITH CAUTION\n",
    "# code_agent = create_react_agent(llm, tools=[python_repl_tool])\n",
    "# code_node = functools.partial(agent_node, agent=code_agent, name=\"Coder\")\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"Appointments\", Appointments_node)\n",
    "workflow.add_node(\"Rag\", Rag_node)\n",
    "workflow.add_node(\"supervisor_agent\", supervisor_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding an edge to a graph that has already been compiled. This will not be reflected in the compiled graph.\n",
      "Adding an edge to a graph that has already been compiled. This will not be reflected in the compiled graph.\n",
      "Adding an edge to a graph that has already been compiled. This will not be reflected in the compiled graph.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Branch with name `None` already exists for node `supervisor_agent`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m conditional_map \u001b[38;5;241m=\u001b[39m {k: k \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m members}\n\u001b[1;32m     12\u001b[0m conditional_map[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFINISH\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m END\n\u001b[0;32m---> 13\u001b[0m \u001b[43mworkflow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_conditional_edges\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msupervisor_agent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnext\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconditional_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Finally, add entrypoint\u001b[39;00m\n\u001b[1;32m     16\u001b[0m workflow\u001b[38;5;241m.\u001b[39madd_edge(START, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupervisor_agent\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Volumes/Ajeet/LLM Projects/LangGraph(KeepMe)/langGEnv/lib/python3.11/site-packages/langgraph/graph/graph.py:293\u001b[0m, in \u001b[0;36mGraph.add_conditional_edges\u001b[0;34m(self, source, path, path_map, then)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;66;03m# validate the condition\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbranches[source]:\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    294\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBranch with name `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` already exists for node \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    295\u001b[0m     )\n\u001b[1;32m    296\u001b[0m \u001b[38;5;66;03m# save it\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbranches[source][name] \u001b[38;5;241m=\u001b[39m Branch(path, path_map_, then)\n",
      "\u001b[0;31mValueError\u001b[0m: Branch with name `None` already exists for node `supervisor_agent`"
     ]
    }
   ],
   "source": [
    "# for member in members:\n",
    "#     # We want our workers to ALWAYS \"report back\" to the supervisor when done\n",
    "#     workflow.add_edge(member, \"supervisor_agent\")\n",
    "workflow.add_edge(\"Appointments\", \"supervisor_agent\")\n",
    "workflow.add_edge(\"Rag\", \"supervisor_agent\")\n",
    "\n",
    "\n",
    "# The supervisor populates the \"next\" field in the graph state\n",
    "# which routes to a node or finishes\n",
    "conditional_map = {k: k for k in members}\n",
    "\n",
    "conditional_map[\"FINISH\"] = END\n",
    "workflow.add_conditional_edges(\"supervisor_agent\", lambda x: x[\"next\"], conditional_map)\n",
    "\n",
    "# Finally, add entrypoint\n",
    "workflow.add_edge(START, \"supervisor_agent\")\n",
    "\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Appointments': 'Appointments', 'Rag': 'Rag', 'FINISH': '__end__'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conditional_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from Agent.agent_take2 import *\n",
    "from Libs.libs import *\n",
    "# from graph import  graph\n",
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "\n",
    "display(Image(graph.get_graph(xray=True).draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'supervisor_agent': {'next': 'Appointments'}}\n",
      "----\n",
      "\n",
      "from check availability by doctor tool\n",
      "desired date is: 2024-09-03\n",
      "\n",
      "---->>>>>>>              date_slot specialization     doctor_name  is_available  \\\n",
      "4262  2024-09-03 08:00   orthodontist  kevin anderson          True   \n",
      "4263  2024-09-03 08:30   orthodontist  kevin anderson          True   \n",
      "4269  2024-09-03 11:30   orthodontist  kevin anderson          True   \n",
      "4270  2024-09-03 12:00   orthodontist  kevin anderson          True   \n",
      "4271  2024-09-03 12:30   orthodontist  kevin anderson          True   \n",
      "4272  2024-09-03 13:00   orthodontist  kevin anderson          True   \n",
      "4274  2024-09-03 14:00   orthodontist  kevin anderson          True   \n",
      "4275  2024-09-03 14:30   orthodontist  kevin anderson          True   \n",
      "4276  2024-09-03 15:00   orthodontist  kevin anderson          True   \n",
      "4277  2024-09-03 15:30   orthodontist  kevin anderson          True   \n",
      "4279  2024-09-03 16:30   orthodontist  kevin anderson          True   \n",
      "\n",
      "     patient_to_attend  \n",
      "4262               NaN  \n",
      "4263               NaN  \n",
      "4269               NaN  \n",
      "4270               NaN  \n",
      "4271               NaN  \n",
      "4272               NaN  \n",
      "4274               NaN  \n",
      "4275               NaN  \n",
      "4276               NaN  \n",
      "4277               NaN  \n",
      "4279               NaN   >>>----- kevin anderson for 2024-09-03,\n",
      "Tool entered with the query kevin anderson for 2024-09-03,\n",
      "------------\n",
      "Hello! I see that you're looking to schedule an appointment with Dr. Kevin Anderson on September 3rd, 2024. I'm happy to inform you that he is available throughout the day with several time slots open. Here are the available times:\n",
      "\n",
      "- 08:00 AM\n",
      "- 08:30 AM\n",
      "- 11:30 AM\n",
      "- 12:00 PM\n",
      "- 12:30 PM\n",
      "- 01:00 PM\n",
      "- 02:00 PM\n",
      "- 02:30 PM\n",
      "- 03:00 PM\n",
      "- 03:30 PM\n",
      "- 04:30 PM\n",
      "\n",
      "Please let me know which time works best for you, and I'll get that scheduled!\n",
      "------------\n",
      "book tool activated 2024-09-03   08:00 AM\n",
      "desired date is: 2024-09-03T08:00 AM\n",
      "08:00 AM\n",
      "{'Appointments': {'messages': [HumanMessage(content='The 8:00 AM time slot is unfortunately not available for Dr. Kevin Anderson. However, here are the other available times on September 3rd, 2024:\\n\\n- 08:30 AM\\n- 11:30 AM\\n- 12:00 PM\\n- 12:30 PM\\n- 01:00 PM\\n- 02:00 PM\\n- 02:30 PM\\n- 03:00 PM\\n- 03:30 PM\\n- 04:30 PM\\n\\nPlease let me know which time works best for you!', additional_kwargs={}, response_metadata={}, name='Appointments')]}}\n",
      "----\n",
      "{'supervisor_agent': {'next': 'Rag'}}\n",
      "----\n",
      "{'Rag': {'messages': [HumanMessage(content='Please let me know which of the following available time slots for Dr. Kevin Anderson on September 3rd, 2024, works best for you:\\n\\n- 08:30 AM\\n- 11:30 AM\\n- 12:00 PM\\n- 12:30 PM\\n- 01:00 PM\\n- 02:00 PM\\n- 02:30 PM\\n- 03:00 PM\\n- 03:30 PM\\n- 04:30 PM', additional_kwargs={}, response_metadata={}, name='Rag')]}}\n",
      "----\n",
      "{'supervisor_agent': {'next': 'Rag'}}\n",
      "----\n",
      "{'Rag': {'messages': [HumanMessage(content='Please let me know which of the following available time slots for Dr. Kevin Anderson on September 3rd, 2024, works best for you:\\n\\n- 08:30 AM\\n- 11:30 AM\\n- 12:00 PM\\n- 12:30 PM\\n- 01:00 PM\\n- 02:00 PM\\n- 02:30 PM\\n- 03:00 PM\\n- 03:30 PM\\n- 04:30 PM', additional_kwargs={}, response_metadata={}, name='Rag')]}}\n",
      "----\n",
      "{'supervisor_agent': {'next': 'FINISH'}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "query = \"Book an appointment with  doctor kevin anderson available on september 3 2024 at 8 aM\"\n",
    "for s in graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=query)\n",
    "        ]\n",
    "    },\n",
    "        {\"recursion_limit\": 100}\n",
    "\n",
    "):\n",
    "    if \"__end__\" not in s:\n",
    "        print(s)\n",
    "        print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from Libs.libs import *\n",
    "\n",
    "\n",
    "from Tools.availability_by_doctor import *\n",
    "from Tools.availability_by_specialization import *\n",
    "from Tools.booking import book_appointment\n",
    "\n",
    "class MessagesState(TypedDict):\n",
    "    messages: Annotated[List[AnyMessage], operator.add]\n",
    "    senderId:str\n",
    "    history:str\n",
    "    \n",
    "    \n",
    "tools_available = [book_appointment,check_availability_by_doctor, check_availability_by_specialization]\n",
    "# tool = [mercedes_tool]\n",
    "tool_node = ToolNode(tools=tools_available)\n",
    "model = llm.bind_tools(tools = tools_available, strict=True)\n",
    "\n",
    "def read_human_feedback(state):\n",
    "    # if state['messages'][-1].tool_calls == []:\n",
    "    #     logger.info(\"AI: \"+ state['messages'][-1].content)\n",
    "    #     user_msg = input(\"Reply: \")\n",
    "    \n",
    "    \n",
    "    # history = {\"history\":[]}\n",
    "    # history = getData(\"abcsdd\")\n",
    "    # if history is None:\n",
    "    #     history = {'history':[]}\n",
    "    # history_new = {\"human_feedback\":f\"{state['messages'][0].content}\", \"ai\":f\"{state['messages'][-1].content}\"}\n",
    "\n",
    "\n",
    "\n",
    "    # history = history_new.append(history['history'])\n",
    "    # history['history'].append(history_new)\n",
    "    # print(history)\n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "    # history['history'].append(history_new)\n",
    "    \n",
    "    # setData(state['senderId'], history)\n",
    "    \n",
    "    # if len(history['history'])>4:\n",
    "    #     length = len(history['history']) - 4\n",
    "    #     for i in range(length):\n",
    "    #         history[\"history\"].pop(0)\n",
    "    \n",
    "    # a = getData(\"abcsdd\")\n",
    "    # combined_history.append(a)\n",
    "    # {\"history\":combined_history}\n",
    "        \n",
    "    \n",
    "    \n",
    "    # print()\n",
    "    # print(state)\n",
    "    # print(\"history\",history)\n",
    "    # print()\n",
    "    return state\n",
    "    #     pass\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"From call_model the state is:\",state['senderId'])\n",
    "    s = getData(state[\"senderId\"])\n",
    "    state[\"history\"] = s\n",
    "    \n",
    "\n",
    "    # history = {\"human_feedback\":state[\"messages\"][1], \"AI\":\"This is ai response\"}\n",
    "    messages = [SystemMessage(content=f\"You are helpful assistant.\\n.As reference, today is {datetime.now().strftime('%Y-%m-%d %H:%M, %A')}\\nKeep a friendly, professional tone.\\nAvoid verbosity.\\nConsiderations:\\n- Dont assume parameters in call functions that it didnt say.\\n- MUST NOT force users how to write. Let them write in the way they want.\\n- The conversation should be very natural like a secretary talking with a client.\\n- Call only ONE tool at a time.\")] + state['messages']\n",
    "    # messages = [SystemMessage(content=\"You are a helpful assistant. As reference, today is {datetime.now().strftime('%Y-%m-%d %H:%M, %A')}. Always use tools to answer the queries\")]\n",
    "\n",
    "    response = model.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def should_continue(state: MessagesState) -> Literal[\"tools\", \"human_feedback\"]:\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    return \"human_feedback\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def should_continue_with_feedback(state: MessagesState) -> Literal[\"agent\", \"end\", \"human_feedback\"]:\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    if isinstance(last_message, dict):\n",
    "        if last_message.get(\"type\",\"\") == 'human_feedback':\n",
    "            return \"agent\"\n",
    "    if (isinstance(last_message, HumanMessage)):\n",
    "        return \"agent\"\n",
    "    if (isinstance(last_message, AIMessage)):\n",
    "        return \"end\"\n",
    "    return \"end\"\n",
    "\n",
    "\n",
    "\n",
    "def graph(query:str, senderId:str):\n",
    "    workflow = StateGraph(MessagesState)\n",
    "    workflow.add_node(\"agent\",call_model)\n",
    "    workflow.add_node(\"tools\",tool_node)\n",
    "    workflow.add_node(\"human_feedback\", read_human_feedback)\n",
    "\n",
    "\n",
    "    workflow.add_conditional_edges(\n",
    "        \"agent\",\n",
    "        should_continue,\n",
    "        {\"human_feedback\":\"human_feedback\",\n",
    "        \"tools\":\"tools\"}\n",
    "    )\n",
    "    workflow.add_conditional_edges(\n",
    "        \"human_feedback\",\n",
    "        should_continue_with_feedback,\n",
    "        {\"agent\":\"agent\",\"end\":END}\n",
    "    )\n",
    "\n",
    "    workflow.add_edge(\"tools\",\"agent\" )\n",
    "\n",
    "\n",
    "    workflow.set_entry_point('agent')\n",
    "\n",
    "\n",
    "\n",
    "    graph = workflow.compile()\n",
    "    inputs = {\"messages\":[HumanMessage(content=query)], \"senderId\":senderId}\n",
    "\n",
    "    for response in graph.stream(inputs):\n",
    "        try:\n",
    "            if \"__end__\" not in response:\n",
    "                print(response)\n",
    "                # if 'human_feedback' in response:\n",
    "                token_usage =response['human_feedback']['messages'][-1].response_metadata['token_usage']\n",
    "                final_response =  response['human_feedback']['messages'][-1].content\n",
    "                    \n",
    "                print(\"-----\")\n",
    "                # history = {\"human_feedback\":query, \"ai\":final_response}\n",
    "                return {\"result\": final_response, \"token_usage\":token_usage}\n",
    "        except Exception as e:\n",
    "            print(\"error\", e)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Tools.tools_init_ import *\n",
    "from Libs.libs import *\n",
    "\n",
    "from Libs.libs import llm\n",
    "from Libs.libs import llm\n",
    "from Agent.supervisor_agent import supervisor_agent, agent_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'supervisor_agent': {'next': 'Appointments'}}\n",
      "----\n",
      "From agent node {'messages': [HumanMessage(content='Book an appointment with  doctor kevin anderson on september 3 2024 at 8 aM', additional_kwargs={}, response_metadata={}, id='6abd0f92-9061-432b-b3e2-363da282a586'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_4ytuesw31LH62kLigQFZaLkR', 'function': {'arguments': '{\"desired_date\":\"2024-09-03\",\"desired_time\":\"08:00\",\"doctor_name\":\"kevin anderson\"}', 'name': 'bookingTool'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 35, 'prompt_tokens': 456, 'total_tokens': 491, 'completion_tokens_details': {'reasoning_tokens': 0}, 'prompt_tokens_details': {'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f85bea6784', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-eafe7f5d-793b-4694-9abc-0bc33f9b772e-0', tool_calls=[{'name': 'bookingTool', 'args': {'desired_date': '2024-09-03', 'desired_time': '08:00', 'doctor_name': 'kevin anderson'}, 'id': 'call_4ytuesw31LH62kLigQFZaLkR', 'type': 'tool_call'}], usage_metadata={'input_tokens': 456, 'output_tokens': 35, 'total_tokens': 491}), ToolMessage(content='Error: AttributeError(\"\\'NoneType\\' object has no attribute \\'senderId\\'\")\\n Please fix your mistakes.', name='bookingTool', id='ff5fb851-da03-47a4-8655-caec9d004691', tool_call_id='call_4ytuesw31LH62kLigQFZaLkR'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_ySU2Np1NZjeEZXOT2WK8iJaG', 'function': {'arguments': '{\"desired_date\":\"2024-09-03T08:00\",\"doctor_name\":\"kevin anderson\"}', 'name': 'checkDoctorAvailabilityTool'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 34, 'prompt_tokens': 520, 'total_tokens': 554, 'completion_tokens_details': {'reasoning_tokens': 0}, 'prompt_tokens_details': {'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f85bea6784', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-8647379a-839f-4ed2-9454-b705883b1df9-0', tool_calls=[{'name': 'checkDoctorAvailabilityTool', 'args': {'desired_date': '2024-09-03T08:00', 'doctor_name': 'kevin anderson'}, 'id': 'call_ySU2Np1NZjeEZXOT2WK8iJaG', 'type': 'tool_call'}], usage_metadata={'input_tokens': 520, 'output_tokens': 34, 'total_tokens': 554}), ToolMessage(content='Error: AttributeError(\"\\'NoneType\\' object has no attribute \\'senderId\\'\")\\n Please fix your mistakes.', name='checkDoctorAvailabilityTool', id='b4843826-7781-44d4-b276-34790d7401df', tool_call_id='call_ySU2Np1NZjeEZXOT2WK8iJaG'), AIMessage(content='It seems there is an issue with the appointment booking system. I am unable to check the availability of Dr. Kevin Anderson or book an appointment at this time.\\n\\nWould you like me to assist you with anything else or try again later?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 48, 'prompt_tokens': 585, 'total_tokens': 633, 'completion_tokens_details': {'reasoning_tokens': 0}, 'prompt_tokens_details': {'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f85bea6784', 'finish_reason': 'stop', 'logprobs': None}, id='run-d03d9a5d-1f68-4fce-b687-282c94355672-0', usage_metadata={'input_tokens': 585, 'output_tokens': 48, 'total_tokens': 633})]}\n",
      "{'Appointments': {'messages': [HumanMessage(content='It seems there is an issue with the appointment booking system. I am unable to check the availability of Dr. Kevin Anderson or book an appointment at this time.\\n\\nWould you like me to assist you with anything else or try again later?', additional_kwargs={}, response_metadata={}, name='Appointments')]}}\n",
      "----\n",
      "{'supervisor_agent': {'next': 'Rag'}}\n",
      "----\n",
      "<><><><><><><><><><><> from RAG Tool\n",
      "[Document(metadata={'pk': 452879034413563618}, page_content='{\"Punch EV\": {\"Empowered LR(Long Range)\": {\"warranty\": {\"Battery_Pack_and_Motor_Warranty\": \"8 Year or 1,60,000 km (whichever is earlier)\"}}}}'),\n",
      " Document(metadata={'pk': 452879034413563667}, page_content='{\"Punch EV\": {\"Empowered+S LR(Long Range)\": {\"warranty\": {\"Battery_Pack_and_Motor_Warranty\": \"8 Year or 1,60,000 km (whichever is earlier)\"}}}}'),\n",
      " Document(metadata={'pk': 452879034413563569}, page_content='{\"Punch EV\": {\"Financing Option / Banking Option\": \"Yes - Up to 80%\"}}'),\n",
      " Document(metadata={'pk': 452879034413563716}, page_content='{\"Punch EV\": {\"Empowered+s MR(Medium Range)\": {\"warranty\": {\"Battery_Pack_and_Motor_Warranty\": \"8 Year or 1,60,000 km (whichever is earlier)\"}}}}')]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m workflow\u001b[38;5;241m.\u001b[39madd_edge(START, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupervisor_agent\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     39\u001b[0m graph \u001b[38;5;241m=\u001b[39m workflow\u001b[38;5;241m.\u001b[39mcompile()\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrecursion_limit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m       \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m__end__\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m           \u001b[49m\u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Volumes/Ajeet/LLM Projects/LangGraph(KeepMe)/langGEnv/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1290\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   1280\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   1282\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   1283\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[1;32m   1284\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   1285\u001b[0m         input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels,\n\u001b[1;32m   1286\u001b[0m         interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before_,\n\u001b[1;32m   1287\u001b[0m         interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after_,\n\u001b[1;32m   1288\u001b[0m         manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[1;32m   1289\u001b[0m     ):\n\u001b[0;32m-> 1290\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1291\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1292\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1293\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1294\u001b[0m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1295\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[1;32m   1297\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[0;32m/Volumes/Ajeet/LLM Projects/LangGraph(KeepMe)/langGEnv/lib/python3.11/site-packages/langgraph/pregel/runner.py:56\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m     54\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 56\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/Volumes/Ajeet/LLM Projects/LangGraph(KeepMe)/langGEnv/lib/python3.11/site-packages/langgraph/pregel/retry.py:29\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy)\u001b[0m\n\u001b[1;32m     27\u001b[0m task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/Volumes/Ajeet/LLM Projects/LangGraph(KeepMe)/langGEnv/lib/python3.11/site-packages/langgraph/utils/runnable.py:385\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 385\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m/Volumes/Ajeet/LLM Projects/LangGraph(KeepMe)/langGEnv/lib/python3.11/site-packages/langgraph/utils/runnable.py:167\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    166\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[0;32m--> 167\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse:\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m/Volumes/Ajeet/LLM Projects/LangGraph(KeepMe)/Agent/supervisor_agent.py:41\u001b[0m, in \u001b[0;36magent_node\u001b[0;34m(state, agent, name)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21magent_node\u001b[39m(state, agent, name):\n\u001b[0;32m---> 41\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFrom agent node\u001b[39m\u001b[38;5;124m\"\u001b[39m, result)\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     44\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [HumanMessage(content\u001b[38;5;241m=\u001b[39mresult[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcontent, name\u001b[38;5;241m=\u001b[39mname)]\n\u001b[1;32m     45\u001b[0m         \u001b[38;5;66;03m# \"messages\": [HumanMessage(content=result[\"messages\"][-1].content, name=name)]\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     }\n",
      "File \u001b[0;32m/Volumes/Ajeet/LLM Projects/LangGraph(KeepMe)/langGEnv/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1551\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, **kwargs)\u001b[0m\n\u001b[1;32m   1549\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1550\u001b[0m     chunks \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m-> 1551\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1552\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1553\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1554\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1555\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1556\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_before\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_before\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1557\u001b[0m \u001b[43m    \u001b[49m\u001b[43minterrupt_after\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minterrupt_after\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1558\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdebug\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdebug\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1559\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1560\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1561\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalues\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1562\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlatest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\n",
      "File \u001b[0;32m/Volumes/Ajeet/LLM Projects/LangGraph(KeepMe)/langGEnv/lib/python3.11/site-packages/langgraph/pregel/__init__.py:1290\u001b[0m, in \u001b[0;36mPregel.stream\u001b[0;34m(self, input, config, stream_mode, output_keys, interrupt_before, interrupt_after, debug, subgraphs)\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[38;5;66;03m# Similarly to Bulk Synchronous Parallel / Pregel model\u001b[39;00m\n\u001b[1;32m   1280\u001b[0m     \u001b[38;5;66;03m# computation proceeds in steps, while there are channel updates\u001b[39;00m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;66;03m# channel updates from step N are only visible in step N+1\u001b[39;00m\n\u001b[1;32m   1282\u001b[0m     \u001b[38;5;66;03m# channels are guaranteed to be immutable for the duration of the step,\u001b[39;00m\n\u001b[1;32m   1283\u001b[0m     \u001b[38;5;66;03m# with channel updates applied only at the transition between steps\u001b[39;00m\n\u001b[1;32m   1284\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtick(\n\u001b[1;32m   1285\u001b[0m         input_keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_channels,\n\u001b[1;32m   1286\u001b[0m         interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before_,\n\u001b[1;32m   1287\u001b[0m         interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after_,\n\u001b[1;32m   1288\u001b[0m         manager\u001b[38;5;241m=\u001b[39mrun_manager,\n\u001b[1;32m   1289\u001b[0m     ):\n\u001b[0;32m-> 1290\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1291\u001b[0m \u001b[43m            \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtasks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1292\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1293\u001b[0m \u001b[43m            \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1294\u001b[0m \u001b[43m            \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1295\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1296\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[1;32m   1297\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1298\u001b[0m \u001b[38;5;66;03m# emit output\u001b[39;00m\n",
      "File \u001b[0;32m/Volumes/Ajeet/LLM Projects/LangGraph(KeepMe)/langGEnv/lib/python3.11/site-packages/langgraph/pregel/runner.py:56\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[0;34m(self, tasks, reraise, timeout, retry_policy, get_waiter)\u001b[0m\n\u001b[1;32m     54\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 56\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[0;32m/Volumes/Ajeet/LLM Projects/LangGraph(KeepMe)/langGEnv/lib/python3.11/site-packages/langgraph/pregel/retry.py:29\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[0;34m(task, retry_policy)\u001b[0m\n\u001b[1;32m     27\u001b[0m task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# if successful, end\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/Volumes/Ajeet/LLM Projects/LangGraph(KeepMe)/langGEnv/lib/python3.11/site-packages/langgraph/utils/runnable.py:385\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, config)\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 385\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[0;32m/Volumes/Ajeet/LLM Projects/LangGraph(KeepMe)/langGEnv/lib/python3.11/site-packages/langchain_core/runnables/base.py:4706\u001b[0m, in \u001b[0;36mRunnableLambda.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4692\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Invoke this Runnable synchronously.\u001b[39;00m\n\u001b[1;32m   4693\u001b[0m \n\u001b[1;32m   4694\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4703\u001b[0m \u001b[38;5;124;03m    TypeError: If the Runnable is a coroutine function.\u001b[39;00m\n\u001b[1;32m   4704\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfunc\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 4706\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_with_config\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4707\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_invoke\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4708\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4709\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4710\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4711\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4712\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   4713\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   4714\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot invoke a coroutine function synchronously.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4715\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse `ainvoke` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   4716\u001b[0m     )\n",
      "File \u001b[0;32m/Volumes/Ajeet/LLM Projects/LangGraph(KeepMe)/langGEnv/lib/python3.11/site-packages/langchain_core/runnables/base.py:1923\u001b[0m, in \u001b[0;36mRunnable._call_with_config\u001b[0;34m(self, func, input, config, run_type, serialized, **kwargs)\u001b[0m\n\u001b[1;32m   1919\u001b[0m     context \u001b[38;5;241m=\u001b[39m copy_context()\n\u001b[1;32m   1920\u001b[0m     context\u001b[38;5;241m.\u001b[39mrun(_set_config_context, child_config)\n\u001b[1;32m   1921\u001b[0m     output \u001b[38;5;241m=\u001b[39m cast(\n\u001b[1;32m   1922\u001b[0m         Output,\n\u001b[0;32m-> 1923\u001b[0m         \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1924\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1925\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1926\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1927\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1928\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1929\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1930\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1931\u001b[0m     )\n\u001b[1;32m   1932\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1933\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/Volumes/Ajeet/LLM Projects/LangGraph(KeepMe)/langGEnv/lib/python3.11/site-packages/langchain_core/runnables/config.py:396\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    395\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 396\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Volumes/Ajeet/LLM Projects/LangGraph(KeepMe)/langGEnv/lib/python3.11/site-packages/langchain_core/runnables/base.py:4562\u001b[0m, in \u001b[0;36mRunnableLambda._invoke\u001b[0;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m   4560\u001b[0m                 output \u001b[38;5;241m=\u001b[39m chunk\n\u001b[1;32m   4561\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4562\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mcall_func_with_variable_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4563\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   4564\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4565\u001b[0m \u001b[38;5;66;03m# If the output is a Runnable, invoke it\u001b[39;00m\n\u001b[1;32m   4566\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, Runnable):\n",
      "File \u001b[0;32m/Volumes/Ajeet/LLM Projects/LangGraph(KeepMe)/langGEnv/lib/python3.11/site-packages/langchain_core/runnables/config.py:396\u001b[0m, in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m accepts_run_manager(func):\n\u001b[1;32m    395\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m run_manager\n\u001b[0;32m--> 396\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Volumes/Ajeet/LLM Projects/LangGraph(KeepMe)/langGEnv/lib/python3.11/site-packages/langgraph/prebuilt/chat_agent_executor.py:444\u001b[0m, in \u001b[0;36mcreate_react_agent.<locals>.call_model\u001b[0;34m(state, config)\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_model\u001b[39m(state: AgentState, config: RunnableConfig) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m AgentState:\n\u001b[0;32m--> 444\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_runnable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    446\u001b[0m         state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_last_step\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, AIMessage)\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mtool_calls\n\u001b[1;32m    449\u001b[0m     ):\n\u001b[1;32m    450\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    451\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m    452\u001b[0m                 AIMessage(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    456\u001b[0m             ]\n\u001b[1;32m    457\u001b[0m         }\n",
      "File \u001b[0;32m/Volumes/Ajeet/LLM Projects/LangGraph(KeepMe)/langGEnv/lib/python3.11/site-packages/langchain_core/runnables/base.py:3022\u001b[0m, in \u001b[0;36mRunnableSequence.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3020\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   3021\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 3022\u001b[0m             \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config)\n\u001b[1;32m   3023\u001b[0m \u001b[38;5;66;03m# finish the root run\u001b[39;00m\n\u001b[1;32m   3024\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/Volumes/Ajeet/LLM Projects/LangGraph(KeepMe)/langGEnv/lib/python3.11/site-packages/langchain_core/runnables/base.py:5343\u001b[0m, in \u001b[0;36mRunnableBindingBase.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   5337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m   5338\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5339\u001b[0m     \u001b[38;5;28minput\u001b[39m: Input,\n\u001b[1;32m   5340\u001b[0m     config: Optional[RunnableConfig] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   5341\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Optional[Any],\n\u001b[1;32m   5342\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Output:\n\u001b[0;32m-> 5343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5344\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5345\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_merge_configs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5346\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5347\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Volumes/Ajeet/LLM Projects/LangGraph(KeepMe)/langGEnv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:284\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minvoke\u001b[39m(\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    275\u001b[0m     \u001b[38;5;28minput\u001b[39m: LanguageModelInput,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    280\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[1;32m    281\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[1;32m    283\u001b[0m         ChatGeneration,\n\u001b[0;32m--> 284\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcallbacks\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    288\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtags\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    289\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_name\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrun_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrun_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m    294\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[0;32m/Volumes/Ajeet/LLM Projects/LangGraph(KeepMe)/langGEnv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:784\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    776\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    777\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    778\u001b[0m     prompts: \u001b[38;5;28mlist\u001b[39m[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    782\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    783\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_messages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Volumes/Ajeet/LLM Projects/LangGraph(KeepMe)/langGEnv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:641\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    639\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n\u001b[1;32m    640\u001b[0m             run_managers[i]\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 641\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    642\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    643\u001b[0m     LLMResult(generations\u001b[38;5;241m=\u001b[39m[res\u001b[38;5;241m.\u001b[39mgenerations], llm_output\u001b[38;5;241m=\u001b[39mres\u001b[38;5;241m.\u001b[39mllm_output)  \u001b[38;5;66;03m# type: ignore[list-item]\u001b[39;00m\n\u001b[1;32m    644\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results\n\u001b[1;32m    645\u001b[0m ]\n\u001b[1;32m    646\u001b[0m llm_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_combine_llm_outputs([res\u001b[38;5;241m.\u001b[39mllm_output \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results])\n",
      "File \u001b[0;32m/Volumes/Ajeet/LLM Projects/LangGraph(KeepMe)/langGEnv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:631\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(messages):\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    630\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 631\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_with_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[43m                \u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    633\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    634\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    636\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    637\u001b[0m         )\n\u001b[1;32m    638\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    639\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/Volumes/Ajeet/LLM Projects/LangGraph(KeepMe)/langGEnv/lib/python3.11/site-packages/langchain_core/language_models/chat_models.py:853\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 853\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    857\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/Volumes/Ajeet/LLM Projects/LangGraph(KeepMe)/langGEnv/lib/python3.11/site-packages/langchain_openai/chat_models/base.py:688\u001b[0m, in \u001b[0;36mBaseChatOpenAI._generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    686\u001b[0m     generation_info \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheaders\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mdict\u001b[39m(raw_response\u001b[38;5;241m.\u001b[39mheaders)}\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 688\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpayload\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    689\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_chat_result(response, generation_info)\n",
      "File \u001b[0;32m/Volumes/Ajeet/LLM Projects/LangGraph(KeepMe)/langGEnv/lib/python3.11/site-packages/openai/_utils/_utils.py:274\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 274\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Volumes/Ajeet/LLM Projects/LangGraph(KeepMe)/langGEnv/lib/python3.11/site-packages/openai/resources/chat/completions.py:704\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, n, parallel_tool_calls, presence_penalty, response_format, seed, service_tier, stop, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    668\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    670\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    701\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    702\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    703\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 704\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    711\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    712\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    713\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    714\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    715\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    716\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    717\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    718\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    719\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    720\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    721\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    723\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    724\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    725\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    726\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    727\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    728\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    729\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    730\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    731\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    732\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    734\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    736\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    737\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    738\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    739\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    740\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    741\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Volumes/Ajeet/LLM Projects/LangGraph(KeepMe)/langGEnv/lib/python3.11/site-packages/openai/_base_client.py:1270\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1257\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1258\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1265\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1266\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1267\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1268\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1269\u001b[0m     )\n\u001b[0;32m-> 1270\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/Volumes/Ajeet/LLM Projects/LangGraph(KeepMe)/langGEnv/lib/python3.11/site-packages/openai/_base_client.py:947\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    945\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 947\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    949\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    950\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    953\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Volumes/Ajeet/LLM Projects/LangGraph(KeepMe)/langGEnv/lib/python3.11/site-packages/openai/_base_client.py:983\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m    980\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSending HTTP Request: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, request\u001b[38;5;241m.\u001b[39mmethod, request\u001b[38;5;241m.\u001b[39murl)\n\u001b[1;32m    982\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 983\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    985\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    986\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    987\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    989\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEncountered httpx.TimeoutException\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/Volumes/Ajeet/LLM Projects/LangGraph(KeepMe)/langGEnv/lib/python3.11/site-packages/httpx/_client.py:926\u001b[0m, in \u001b[0;36mClient.send\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_timeout(request)\n\u001b[1;32m    924\u001b[0m auth \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_request_auth(request, auth)\n\u001b[0;32m--> 926\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    931\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    932\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    933\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "File \u001b[0;32m/Volumes/Ajeet/LLM Projects/LangGraph(KeepMe)/langGEnv/lib/python3.11/site-packages/httpx/_client.py:954\u001b[0m, in \u001b[0;36mClient._send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    951\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[1;32m    953\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 954\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    956\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    959\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    960\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/Volumes/Ajeet/LLM Projects/LangGraph(KeepMe)/langGEnv/lib/python3.11/site-packages/httpx/_client.py:991\u001b[0m, in \u001b[0;36mClient._send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequest\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    989\u001b[0m     hook(request)\n\u001b[0;32m--> 991\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    992\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    993\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_hooks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m/Volumes/Ajeet/LLM Projects/LangGraph(KeepMe)/langGEnv/lib/python3.11/site-packages/httpx/_client.py:1027\u001b[0m, in \u001b[0;36mClient._send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1023\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1024\u001b[0m     )\n\u001b[1;32m   1026\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request\u001b[38;5;241m=\u001b[39mrequest):\n\u001b[0;32m-> 1027\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, SyncByteStream)\n\u001b[1;32m   1031\u001b[0m response\u001b[38;5;241m.\u001b[39mrequest \u001b[38;5;241m=\u001b[39m request\n",
      "File \u001b[0;32m/Volumes/Ajeet/LLM Projects/LangGraph(KeepMe)/langGEnv/lib/python3.11/site-packages/httpx/_transports/default.py:236\u001b[0m, in \u001b[0;36mHTTPTransport.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    223\u001b[0m req \u001b[38;5;241m=\u001b[39m httpcore\u001b[38;5;241m.\u001b[39mRequest(\n\u001b[1;32m    224\u001b[0m     method\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mmethod,\n\u001b[1;32m    225\u001b[0m     url\u001b[38;5;241m=\u001b[39mhttpcore\u001b[38;5;241m.\u001b[39mURL(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    233\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mrequest\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    234\u001b[0m )\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[0;32m--> 236\u001b[0m     resp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp\u001b[38;5;241m.\u001b[39mstream, typing\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[1;32m    241\u001b[0m     status_code\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mstatus,\n\u001b[1;32m    242\u001b[0m     headers\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mheaders,\n\u001b[1;32m    243\u001b[0m     stream\u001b[38;5;241m=\u001b[39mResponseStream(resp\u001b[38;5;241m.\u001b[39mstream),\n\u001b[1;32m    244\u001b[0m     extensions\u001b[38;5;241m=\u001b[39mresp\u001b[38;5;241m.\u001b[39mextensions,\n\u001b[1;32m    245\u001b[0m )\n",
      "File \u001b[0;32m/Volumes/Ajeet/LLM Projects/LangGraph(KeepMe)/langGEnv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:216\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    213\u001b[0m         closing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_requests_to_connections()\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_close_connections(closing)\n\u001b[0;32m--> 216\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response\u001b[38;5;241m.\u001b[39mstream, Iterable)\n",
      "File \u001b[0;32m/Volumes/Ajeet/LLM Projects/LangGraph(KeepMe)/langGEnv/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py:196\u001b[0m, in \u001b[0;36mConnectionPool.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    192\u001b[0m connection \u001b[38;5;241m=\u001b[39m pool_request\u001b[38;5;241m.\u001b[39mwait_for_connection(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[0;32m--> 196\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    203\u001b[0m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[1;32m    204\u001b[0m     pool_request\u001b[38;5;241m.\u001b[39mclear_connection()\n",
      "File \u001b[0;32m/Volumes/Ajeet/LLM Projects/LangGraph(KeepMe)/langGEnv/lib/python3.11/site-packages/httpcore/_sync/connection.py:101\u001b[0m, in \u001b[0;36mHTTPConnection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_connect_failed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_connection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Volumes/Ajeet/LLM Projects/LangGraph(KeepMe)/langGEnv/lib/python3.11/site-packages/httpcore/_sync/http11.py:143\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse_closed\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    142\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response_closed()\n\u001b[0;32m--> 143\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "File \u001b[0;32m/Volumes/Ajeet/LLM Projects/LangGraph(KeepMe)/langGEnv/lib/python3.11/site-packages/httpcore/_sync/http11.py:113\u001b[0m, in \u001b[0;36mHTTP11Connection.handle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreceive_response_headers\u001b[39m\u001b[38;5;124m\"\u001b[39m, logger, request, kwargs\n\u001b[1;32m    106\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[1;32m    107\u001b[0m     (\n\u001b[1;32m    108\u001b[0m         http_version,\n\u001b[1;32m    109\u001b[0m         status,\n\u001b[1;32m    110\u001b[0m         reason_phrase,\n\u001b[1;32m    111\u001b[0m         headers,\n\u001b[1;32m    112\u001b[0m         trailing_data,\n\u001b[0;32m--> 113\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m     trace\u001b[38;5;241m.\u001b[39mreturn_value \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    115\u001b[0m         http_version,\n\u001b[1;32m    116\u001b[0m         status,\n\u001b[1;32m    117\u001b[0m         reason_phrase,\n\u001b[1;32m    118\u001b[0m         headers,\n\u001b[1;32m    119\u001b[0m     )\n\u001b[1;32m    121\u001b[0m network_stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_network_stream\n",
      "File \u001b[0;32m/Volumes/Ajeet/LLM Projects/LangGraph(KeepMe)/langGEnv/lib/python3.11/site-packages/httpcore/_sync/http11.py:186\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    183\u001b[0m timeout \u001b[38;5;241m=\u001b[39m timeouts\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 186\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11\u001b[38;5;241m.\u001b[39mResponse):\n\u001b[1;32m    188\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/Volumes/Ajeet/LLM Projects/LangGraph(KeepMe)/langGEnv/lib/python3.11/site-packages/httpcore/_sync/http11.py:224\u001b[0m, in \u001b[0;36mHTTP11Connection._receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    221\u001b[0m     event \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mnext_event()\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11\u001b[38;5;241m.\u001b[39mNEED_DATA:\n\u001b[0;32m--> 224\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_network_stream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    230\u001b[0m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;241m==\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_h11_state\u001b[38;5;241m.\u001b[39mtheir_state \u001b[38;5;241m==\u001b[39m h11\u001b[38;5;241m.\u001b[39mSEND_RESPONSE:\n",
      "File \u001b[0;32m/Volumes/Ajeet/LLM Projects/LangGraph(KeepMe)/langGEnv/lib/python3.11/site-packages/httpcore/_backends/sync.py:126\u001b[0m, in \u001b[0;36mSyncStream.read\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock\u001b[38;5;241m.\u001b[39msettimeout(timeout)\n\u001b[0;32m--> 126\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Volumes/Ajeet/LLM Projects/LangGraph(KeepMe)/langGEnv/lib/python3.11/ssl.py:1263\u001b[0m, in \u001b[0;36mSSLSocket.recv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1259\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m flags \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1260\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1261\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[1;32m   1262\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n\u001b[0;32m-> 1263\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1264\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mrecv(buflen, flags)\n",
      "File \u001b[0;32m/Volumes/Ajeet/LLM Projects/LangGraph(KeepMe)/langGEnv/lib/python3.11/ssl.py:1136\u001b[0m, in \u001b[0;36mSSLSocket.read\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sslobj\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[1;32m   1135\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1136\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sslobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[1;32m   1138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39margs[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuppress_ragged_eofs:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from Redis.utilis import RedisSaver\n",
    "from Tools.ragAgent import rag_tool\n",
    "\n",
    "from Tools.tools_init_ import *\n",
    "from Libs.libs import *\n",
    "\n",
    "from Libs.libs import llm\n",
    "from Libs.libs import llm\n",
    "from Agent.supervisor_agent import supervisor_agent, agent_node\n",
    "\n",
    "# def graph_main(query:str, senderId:str):\n",
    "\n",
    "\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    # The annotation tells the graph that new messages will always\n",
    "    # be added to the current states\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    # The 'next' field indicates where to route to next\n",
    "    next: str\n",
    "query = \"Book an appointment with  doctor kevin anderson on september 3 2024 at 8 aM\"\n",
    "\n",
    "toolsInstance = GetCustomTools()\n",
    "tools_available = toolsInstance.get_tools()\n",
    "\n",
    "Appointments = create_react_agent(llm, tools=tools_available, )\n",
    "Appointments_node = functools.partial(agent_node, agent=Appointments, name=\"Appointments\")\n",
    "\n",
    "Rag = create_react_agent(llm, tools=[rag_tool])\n",
    "Rag_node = functools.partial(agent_node, agent=Rag, name=\"Rag\")\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"Appointments\", Appointments_node)\n",
    "workflow.add_node(\"Rag\", Rag_node)\n",
    "workflow.add_node(\"supervisor_agent\", supervisor_agent)\n",
    "workflow.add_edge(\"Appointments\", \"supervisor_agent\")\n",
    "conditional_map = {'Appointments': 'Appointments', 'Rag': 'Rag', 'FINISH': '__end__'}\n",
    "workflow.add_conditional_edges(\"supervisor_agent\", lambda x: x[\"next\"], conditional_map)\n",
    "workflow.add_edge(START, \"supervisor_agent\")\n",
    "graph = workflow.compile()\n",
    "\n",
    "for s in graph.stream({\"messages\": [HumanMessage(content=query)]},{\"recursion_limit\": 10}):\n",
    "       if \"__end__\" not in s:\n",
    "           print(s)\n",
    "           print(\"----\")\n",
    "# with RedisSaver.from_conn_info(host=\"localhost\", port=6379, db=0) as checkpointer:\n",
    "#     graph = workflow.compile(checkpointer=checkpointer)\n",
    "#     inputs = {\"messages\": [HumanMessage(content=query)], \"senderId\": \"senderId\"}\n",
    "#     config = {\"configurable\": {\"thread_id\": \"senderId\"}}\n",
    "\n",
    "#     for s in graph.stream({\"messages\": [HumanMessage(content=query)]},{\"recursion_limit\": 100}):\n",
    "#        if \"__end__\" not in s:\n",
    "#            print(s)\n",
    "#            print(\"----\")\n",
    "                \n",
    "                \n",
    "                \n",
    "                \n",
    "#                 # print({\"result\": final_response, \"token_usage\": token_usage})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
