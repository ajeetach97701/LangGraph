{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated\n",
    "\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langchain_experimental.tools import PythonREPLTool\n",
    "\n",
    "from Libs.libs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lamborghini Tool\n",
    "from Schema import *\n",
    "def Lamborghini(query: str):\n",
    "    template = \"\"\"\n",
    "    You are a tool that answer queries related to Lamborghini vehicle. \n",
    "    If the query is other than Lamborghini, say i do not know.\n",
    "    You are given a human query: {query}\n",
    "\n",
    "        provide response in 3 sentences in under 30 words and  concise, precise.\n",
    "\n",
    "\n",
    "                    \"\"\"\n",
    "    prompt_temp = ChatPromptTemplate.from_template(template=template\n",
    "                                                   )\n",
    "\n",
    "    chain = RunnableMap({\n",
    "        \"query\": lambda x: x['query'],\n",
    "\n",
    "    }) | prompt_temp | llm | string_parser\n",
    "    \n",
    "\n",
    "    response = chain.invoke({\"query\": query})\n",
    "    print(response)\n",
    "    return response\n",
    "\n",
    "\n",
    "\n",
    "vehicle_lambo_Tool = StructuredTool.from_function(\n",
    "        name='vehicleLamboTool',\n",
    "        func=Lamborghini,\n",
    "        description=\"A tool that responds to query related to only Lamborghini.\",\n",
    "        args_schema=QueryInput,\n",
    "        return_direct=True\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def mercedes(query: str):\n",
    "    template = \"\"\"\n",
    "    You are a tool that answer queries related to specs of mercedes vehicle. \n",
    "    If the query is other than mercedes, say i do not knoe\n",
    "    You are given a human query: {query}\n",
    "         provide response in 3 sentences in under 30 words and  concise, precise.\n",
    "                    \"\"\"\n",
    "    prompt_temp = ChatPromptTemplate.from_template(template=template\n",
    "                                                   )\n",
    "\n",
    "    chain = RunnableMap({\n",
    "        \"query\": lambda x: x['query'],\n",
    "\n",
    "    }) | prompt_temp | llm |string_parser\n",
    "\n",
    "    response = chain.invoke({\"query\": query})\n",
    "    print(response)\n",
    "    return response\n",
    "\n",
    "\n",
    "\n",
    "vehicleTool = StructuredTool.from_function(\n",
    "        name='vehicleTool',\n",
    "        func=mercedes,\n",
    "        description=\"A tool that responds to query related to only Mercedes.\",\n",
    "        args_schema=QueryInput,\n",
    "        return_direct=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools_available = [vehicle_lambo_Tool, vehicleTool]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "\n",
    "def agent_node(state, agent, name):\n",
    "    result = agent.invoke(state)\n",
    "    return {\n",
    "        \"messages\": [HumanMessage(content=result[\"messages\"][-1].content, name=name)]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervisor agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_openai import ChatOpenAI\n",
    "from pydantic import BaseModel\n",
    "from typing import Literal\n",
    "\n",
    "members = [\"Mercedes\", \"Lamborghini\"]\n",
    "system_prompt = (\n",
    "    \"You are a supervisor tasked with managing a conversation between the\"\n",
    "    \" following workers:  {members}. Given the following user request,\"\n",
    "    \" respond with the worker to act next. Each worker will perform a\"\n",
    "    \" task and respond with their results and status. When finished,\"\n",
    "    \" respond with FINISH.\"\n",
    ")\n",
    "# Our team supervisor is an LLM node. It just picks the next agent to process\n",
    "# and decides when the work is completed\n",
    "options = [\"FINISH\"] + members\n",
    "\n",
    "\n",
    "class routeResponse(BaseModel):\n",
    "    next: Literal[\"Mercedes\", \"Lamborghini\", \"FINISH\"]\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(variable_name=\"messages\"),\n",
    "        (\n",
    "            \"system\",\n",
    "            \"Given the conversation above, who should act next?\"\n",
    "            \" Or should we FINISH? Select one of: {options}\",\n",
    "        ),\n",
    "    ]\n",
    ").partial(options=str(options), members=\", \".join(members))\n",
    "\n",
    "\n",
    "# llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "\n",
    "def supervisor_agent(state):\n",
    "    supervisor_chain = prompt | llm.with_structured_output(routeResponse)\n",
    "    return supervisor_chain.invoke(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x12a136e10>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import functools\n",
    "import operator\n",
    "from typing import Sequence\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langchain_core.messages import BaseMessage\n",
    "\n",
    "from langgraph.graph import END, StateGraph, START\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "\n",
    "# The agent state is the input to each node in the graph\n",
    "class AgentState(TypedDict):\n",
    "    # The annotation tells the graph that new messages will always\n",
    "    # be added to the current states\n",
    "    messages: Annotated[Sequence[BaseMessage], operator.add]\n",
    "    # The 'next' field indicates where to route to next\n",
    "    next: str\n",
    "\n",
    "\n",
    "mercedes = create_react_agent(llm, tools=[vehicleTool])\n",
    "lamborghini = create_react_agent(llm, tools=[vehicle_lambo_Tool])\n",
    "lamborghini_node = functools.partial(agent_node, agent=mercedes, name=\"Mercedes\")\n",
    "lamborghini_node = functools.partial(agent_node, agent=lamborghini, name=\"Lamborghini\")\n",
    "\n",
    "# # NOTE: THIS PERFORMS ARBITRARY CODE EXECUTION. PROCEED WITH CAUTION\n",
    "# code_agent = create_react_agent(llm, tools=[python_repl_tool])\n",
    "# code_node = functools.partial(agent_node, agent=code_agent, name=\"Coder\")\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"Lamborghini\", lamborghini_node)\n",
    "workflow.add_node(\"Mercedes\", mercedes)\n",
    "workflow.add_node(\"supervisor\", supervisor_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for member in members:\n",
    "    # We want our workers to ALWAYS \"report back\" to the supervisor when done\n",
    "    workflow.add_edge(member, \"supervisor\")\n",
    "# The supervisor populates the \"next\" field in the graph state\n",
    "# which routes to a node or finishes\n",
    "conditional_map = {k: k for k in members}\n",
    "conditional_map[\"FINISH\"] = END\n",
    "workflow.add_conditional_edges(\"supervisor\", lambda x: x[\"next\"], conditional_map)\n",
    "# Finally, add entrypoint\n",
    "workflow.add_edge(START, \"supervisor\")\n",
    "\n",
    "graph = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'supervisor': {'next': 'Mercedes'}}\n",
      "----\n",
      "Mercedes-Benz is a luxury automotive brand known for its high-quality vehicles, innovative technology, and performance. The brand offers a wide range of cars, SUVs, and electric vehicles. It combines elegance with cutting-edge engineering.\n",
      "{'Mercedes': {'messages': [HumanMessage(content='Tell me about mercedes', additional_kwargs={}, response_metadata={}, id='7e2d37b9-ffd6-4e86-b6ab-48f2fcebb4f4'), AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_jXwkqn3jl90F055AMEpnG8To', 'function': {'arguments': '{\"query\":\"Tell me about Mercedes.\"}', 'name': 'vehicleTool'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 66, 'total_tokens': 84, 'completion_tokens_details': {'reasoning_tokens': 0}, 'prompt_tokens_details': {'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f85bea6784', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run-f69620b7-91e1-419b-9ca5-845918e45ee9-0', tool_calls=[{'name': 'vehicleTool', 'args': {'query': 'Tell me about Mercedes.'}, 'id': 'call_jXwkqn3jl90F055AMEpnG8To', 'type': 'tool_call'}], usage_metadata={'input_tokens': 66, 'output_tokens': 18, 'total_tokens': 84}), ToolMessage(content='Mercedes-Benz is a luxury automotive brand known for its high-quality vehicles, innovative technology, and performance. The brand offers a wide range of cars, SUVs, and electric vehicles. It combines elegance with cutting-edge engineering.', name='vehicleTool', id='01efd399-dd83-4d0f-8e95-61d7f60abeb2', tool_call_id='call_jXwkqn3jl90F055AMEpnG8To')]}}\n",
      "----\n",
      "{'supervisor': {'next': 'Lamborghini'}}\n",
      "----\n",
      "{'Lamborghini': {'messages': [HumanMessage(content='Mercedes-Benz is a luxury automotive brand renowned for its high-quality vehicles, innovative technology, and performance. The brand offers a diverse lineup that includes cars, SUVs, and electric vehicles, combining elegance with cutting-edge engineering.', additional_kwargs={}, response_metadata={}, name='Lamborghini')]}}\n",
      "----\n",
      "{'supervisor': {'next': 'FINISH'}}\n",
      "----\n"
     ]
    }
   ],
   "source": [
    "for s in graph.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            HumanMessage(content=\"Tell me about mercedes\")\n",
    "        ]\n",
    "    }\n",
    "):\n",
    "    if \"__end__\" not in s:\n",
    "        print(s)\n",
    "        print(\"----\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "from Libs.libs import *\n",
    "\n",
    "\n",
    "from Tools.availability_by_doctor import *\n",
    "from Tools.availability_by_specialization import *\n",
    "from Tools.booking import book_appointment\n",
    "\n",
    "class MessagesState(TypedDict):\n",
    "    messages: Annotated[List[AnyMessage], operator.add]\n",
    "    senderId:str\n",
    "    history:str\n",
    "    \n",
    "    \n",
    "tools_available = [book_appointment,check_availability_by_doctor, check_availability_by_specialization]\n",
    "# tool = [mercedes_tool]\n",
    "tool_node = ToolNode(tools=tools_available)\n",
    "model = llm.bind_tools(tools = tools_available, strict=True)\n",
    "\n",
    "def read_human_feedback(state):\n",
    "    # if state['messages'][-1].tool_calls == []:\n",
    "    #     logger.info(\"AI: \"+ state['messages'][-1].content)\n",
    "    #     user_msg = input(\"Reply: \")\n",
    "    \n",
    "    \n",
    "    # history = {\"history\":[]}\n",
    "    # history = getData(\"abcsdd\")\n",
    "    # if history is None:\n",
    "    #     history = {'history':[]}\n",
    "    # history_new = {\"human_feedback\":f\"{state['messages'][0].content}\", \"ai\":f\"{state['messages'][-1].content}\"}\n",
    "\n",
    "\n",
    "\n",
    "    # history = history_new.append(history['history'])\n",
    "    # history['history'].append(history_new)\n",
    "    # print(history)\n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "    # history['history'].append(history_new)\n",
    "    \n",
    "    # setData(state['senderId'], history)\n",
    "    \n",
    "    # if len(history['history'])>4:\n",
    "    #     length = len(history['history']) - 4\n",
    "    #     for i in range(length):\n",
    "    #         history[\"history\"].pop(0)\n",
    "    \n",
    "    # a = getData(\"abcsdd\")\n",
    "    # combined_history.append(a)\n",
    "    # {\"history\":combined_history}\n",
    "        \n",
    "    \n",
    "    \n",
    "    # print()\n",
    "    # print(state)\n",
    "    # print(\"history\",history)\n",
    "    # print()\n",
    "    return state\n",
    "    #     pass\n",
    "\n",
    "def call_model(state: MessagesState):\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"From call_model the state is:\",state['senderId'])\n",
    "    s = getData(state[\"senderId\"])\n",
    "    state[\"history\"] = s\n",
    "    \n",
    "\n",
    "    # history = {\"human_feedback\":state[\"messages\"][1], \"AI\":\"This is ai response\"}\n",
    "    messages = [SystemMessage(content=f\"You are helpful assistant.\\n.As reference, today is {datetime.now().strftime('%Y-%m-%d %H:%M, %A')}\\nKeep a friendly, professional tone.\\nAvoid verbosity.\\nConsiderations:\\n- DonÂ´t assume parameters in call functions that it didnt say.\\n- MUST NOT force users how to write. Let them write in the way they want.\\n- The conversation should be very natural like a secretary talking with a client.\\n- Call only ONE tool at a time.\")] + state['messages']\n",
    "    # messages = [SystemMessage(content=\"You are a helpful assistant. As reference, today is {datetime.now().strftime('%Y-%m-%d %H:%M, %A')}. Always use tools to answer the queries\")]\n",
    "\n",
    "    response = model.invoke(messages)\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "def should_continue(state: MessagesState) -> Literal[\"tools\", \"human_feedback\"]:\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    if last_message.tool_calls:\n",
    "        return \"tools\"\n",
    "    return \"human_feedback\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def should_continue_with_feedback(state: MessagesState) -> Literal[\"agent\", \"end\", \"human_feedback\"]:\n",
    "    messages = state['messages']\n",
    "    last_message = messages[-1]\n",
    "    if isinstance(last_message, dict):\n",
    "        if last_message.get(\"type\",\"\") == 'human_feedback':\n",
    "            return \"agent\"\n",
    "    if (isinstance(last_message, HumanMessage)):\n",
    "        return \"agent\"\n",
    "    if (isinstance(last_message, AIMessage)):\n",
    "        return \"end\"\n",
    "    return \"end\"\n",
    "\n",
    "\n",
    "\n",
    "def graph(query:str, senderId:str):\n",
    "    workflow = StateGraph(MessagesState)\n",
    "    workflow.add_node(\"agent\",call_model)\n",
    "    workflow.add_node(\"tools\",tool_node)\n",
    "    workflow.add_node(\"human_feedback\", read_human_feedback)\n",
    "\n",
    "\n",
    "    workflow.add_conditional_edges(\n",
    "        \"agent\",\n",
    "        should_continue,\n",
    "        {\"human_feedback\":\"human_feedback\",\n",
    "        \"tools\":\"tools\"}\n",
    "    )\n",
    "    workflow.add_conditional_edges(\n",
    "        \"human_feedback\",\n",
    "        should_continue_with_feedback,\n",
    "        {\"agent\":\"agent\",\"end\":END}\n",
    "    )\n",
    "\n",
    "    workflow.add_edge(\"tools\",\"agent\" )\n",
    "\n",
    "\n",
    "    workflow.set_entry_point('agent')\n",
    "\n",
    "\n",
    "\n",
    "    graph = workflow.compile()\n",
    "    inputs = {\"messages\":[HumanMessage(content=query)], \"senderId\":senderId}\n",
    "\n",
    "    for response in graph.stream(inputs):\n",
    "        try:\n",
    "            if \"__end__\" not in response:\n",
    "                print(response)\n",
    "                # if 'human_feedback' in response:\n",
    "                token_usage =response['human_feedback']['messages'][-1].response_metadata['token_usage']\n",
    "                final_response =  response['human_feedback']['messages'][-1].content\n",
    "                    \n",
    "                print(\"-----\")\n",
    "                # history = {\"human_feedback\":query, \"ai\":final_response}\n",
    "                return {\"result\": final_response, \"token_usage\":token_usage}\n",
    "        except Exception as e:\n",
    "            print(\"error\", e)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
